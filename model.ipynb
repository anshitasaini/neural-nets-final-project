{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Final Project -- Jeopardy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block to load important libraries and set things up\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up / data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216930, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show Number</th>\n",
       "      <th>Air Date</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>McDonald's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>John Adams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Show Number    Air Date      Round                         Category  Value  \\\n",
       "0         4680  2004-12-31  Jeopardy!                          HISTORY   $200   \n",
       "1         4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
       "2         4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
       "3         4680  2004-12-31  Jeopardy!                 THE COMPANY LINE   $200   \n",
       "4         4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES   $200   \n",
       "\n",
       "                                            Question      Answer  \n",
       "0  For the last 8 years of his life, Galileo was ...  Copernicus  \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  \n",
       "2  The city of Yuma in this state has a record av...     Arizona  \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's  \n",
       "4  Signer of the Dec. of Indep., framer of the Co...  John Adams  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv('JEOPARDY_CSV.csv')\n",
    "\n",
    "print(data_df.shape)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR DEBUGGING\n",
    "data_df = data_df.iloc[:10000,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### polish & remove outlier data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210452, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show Number</th>\n",
       "      <th>Air Date</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>200</td>\n",
       "      <td>for the last 8 years of his life, galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>200</td>\n",
       "      <td>no. 2: 1912 olympian; football star at carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>200</td>\n",
       "      <td>the city of yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>200</td>\n",
       "      <td>in 1963, live on \"the art linkletter show\", th...</td>\n",
       "      <td>McDonald's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>200</td>\n",
       "      <td>signer of the dec. of indep., framer of the co...</td>\n",
       "      <td>John Adams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216924</th>\n",
       "      <td>4999</td>\n",
       "      <td>2006-05-11</td>\n",
       "      <td>Double Jeopardy!</td>\n",
       "      <td>OFF-BROADWAY</td>\n",
       "      <td>2000</td>\n",
       "      <td>in 2006 the cast of this long-running hit emba...</td>\n",
       "      <td>Stomp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216925</th>\n",
       "      <td>4999</td>\n",
       "      <td>2006-05-11</td>\n",
       "      <td>Double Jeopardy!</td>\n",
       "      <td>RIDDLE ME THIS</td>\n",
       "      <td>2000</td>\n",
       "      <td>this puccini opera turns on the solution to 3 ...</td>\n",
       "      <td>Turandot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216926</th>\n",
       "      <td>4999</td>\n",
       "      <td>2006-05-11</td>\n",
       "      <td>Double Jeopardy!</td>\n",
       "      <td>\"T\" BIRDS</td>\n",
       "      <td>2000</td>\n",
       "      <td>in north america this term is properly applied...</td>\n",
       "      <td>a titmouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216927</th>\n",
       "      <td>4999</td>\n",
       "      <td>2006-05-11</td>\n",
       "      <td>Double Jeopardy!</td>\n",
       "      <td>AUTHORS IN THEIR YOUTH</td>\n",
       "      <td>2000</td>\n",
       "      <td>in penny lane, where this \"hellraiser\" grew up...</td>\n",
       "      <td>Clive Barker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216928</th>\n",
       "      <td>4999</td>\n",
       "      <td>2006-05-11</td>\n",
       "      <td>Double Jeopardy!</td>\n",
       "      <td>QUOTATIONS</td>\n",
       "      <td>2000</td>\n",
       "      <td>from ft. sill, okla. he made the plea, arizona...</td>\n",
       "      <td>Geronimo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210452 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Show Number    Air Date             Round  \\\n",
       "0              4680  2004-12-31         Jeopardy!   \n",
       "1              4680  2004-12-31         Jeopardy!   \n",
       "2              4680  2004-12-31         Jeopardy!   \n",
       "3              4680  2004-12-31         Jeopardy!   \n",
       "4              4680  2004-12-31         Jeopardy!   \n",
       "...             ...         ...               ...   \n",
       "216924         4999  2006-05-11  Double Jeopardy!   \n",
       "216925         4999  2006-05-11  Double Jeopardy!   \n",
       "216926         4999  2006-05-11  Double Jeopardy!   \n",
       "216927         4999  2006-05-11  Double Jeopardy!   \n",
       "216928         4999  2006-05-11  Double Jeopardy!   \n",
       "\n",
       "                               Category   Value  \\\n",
       "0                               HISTORY     200   \n",
       "1       ESPN's TOP 10 ALL-TIME ATHLETES     200   \n",
       "2           EVERYBODY TALKS ABOUT IT...     200   \n",
       "3                      THE COMPANY LINE     200   \n",
       "4                   EPITAPHS & TRIBUTES     200   \n",
       "...                                 ...     ...   \n",
       "216924                     OFF-BROADWAY    2000   \n",
       "216925                   RIDDLE ME THIS    2000   \n",
       "216926                        \"T\" BIRDS    2000   \n",
       "216927           AUTHORS IN THEIR YOUTH    2000   \n",
       "216928                       QUOTATIONS    2000   \n",
       "\n",
       "                                                 Question        Answer  \n",
       "0       for the last 8 years of his life, galileo was ...    Copernicus  \n",
       "1       no. 2: 1912 olympian; football star at carlisl...    Jim Thorpe  \n",
       "2       the city of yuma in this state has a record av...       Arizona  \n",
       "3       in 1963, live on \"the art linkletter show\", th...    McDonald's  \n",
       "4       signer of the dec. of indep., framer of the co...    John Adams  \n",
       "...                                                   ...           ...  \n",
       "216924  in 2006 the cast of this long-running hit emba...         Stomp  \n",
       "216925  this puccini opera turns on the solution to 3 ...      Turandot  \n",
       "216926  in north america this term is properly applied...    a titmouse  \n",
       "216927  in penny lane, where this \"hellraiser\" grew up...  Clive Barker  \n",
       "216928  from ft. sill, okla. he made the plea, arizona...      Geronimo  \n",
       "\n",
       "[210452 rows x 7 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = data_df[data_df[' Value'].notnull()] # remove null values \n",
    "\n",
    "def extract_val(x):\n",
    "    x = x.replace(',', '')\n",
    "    x = x.replace('$', '')\n",
    "    return int(x)\n",
    "\n",
    "values = data_df[' Value'].apply(extract_val) # converting value strings to ints\n",
    "data_df[' Value'] = values\n",
    "\n",
    "data_df = data_df[data_df[' Value'] <= 2000] # removing data with values over 2000 #TODO: bin value data?\n",
    "\n",
    "data_df[' Question'] = data_df[' Question'].str.lower() # no repeat characters\n",
    "print(data_df.shape)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make vocab dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ö': 0, 'e': 1, '-': 2, 'ç': 3, 'm': 4, '>': 5, 'ê': 6, '£': 7, '^': 8, '#': 9, '¿': 10, '–': 11, '3': 12, 'ì': 13, 's': 14, '¾': 15, '<': 16, 'à': 17, 'r': 18, '0': 19, 'x': 20, '7': 21, ' ': 22, 'í': 23, 'j': 24, '@': 25, 'ô': 26, '|': 27, '5': 28, \"'\": 29, '’': 30, 'u': 31, 'è': 32, '\"': 33, 'ñ': 34, '¢': 35, '(': 36, 'v': 37, 'k': 38, 'h': 39, '.': 40, '=': 41, ';': 42, 'd': 43, 'q': 44, '6': 45, 'o': 46, '?': 47, '[': 48, ',': 49, 'î': 50, 'z': 51, '8': 52, '1': 53, ')': 54, '$': 55, ']': 56, 'ü': 57, 'é': 58, 'g': 59, 'â': 60, 't': 61, 'f': 62, '‘': 63, '”': 64, 'y': 65, '2': 66, '9': 67, '&': 68, 'b': 69, '+': 70, 'p': 71, 'l': 72, '4': 73, 'c': 74, '—': 75, 'ã': 76, '“': 77, '*': 78, '½': 79, '`': 80, '…': 81, '°': 82, 'a': 83, 'w': 84, '!': 85, 'i': 86, '%': 87, 'á': 88, '²': 89, 'n': 90, '/': 91, 'ó': 92, '_': 93, 'º': 94, '¼': 95, 'å': 96, 'ë': 97, ':': 98}\n",
      "Characters: {'ö', 'e', '-', 'ç', 'm', '>', 'ê', '£', '^', '#', '¿', '–', '3', 'ì', 's', '¾', '<', 'à', 'r', '0', 'x', '7', ' ', 'í', 'j', '@', 'ô', '|', '5', \"'\", '’', 'u', 'è', '\"', 'ñ', '¢', '(', 'v', 'k', 'h', '.', '=', ';', 'd', 'q', '6', 'o', '?', '[', ',', 'î', 'z', '8', '1', ')', '$', ']', 'ü', 'é', 'g', 'â', 't', 'f', '‘', '”', 'y', '2', '9', '&', 'b', '+', 'p', 'l', '4', 'c', '—', 'ã', '“', '*', '½', '`', '…', '°', 'a', 'w', '!', 'i', '%', 'á', '²', 'n', '/', 'ó', '_', 'º', '¼', 'å', 'ë', ':'}\n",
      "Number of unique characters: 99\n",
      "All possible values: [5, 20, 22, 50, 100, 200, 250, 300, 350, 367, 400, 500, 585, 600, 601, 700, 750, 796, 800, 900, 1000, 1020, 1100, 1111, 1183, 1200, 1203, 1246, 1263, 1300, 1347, 1400, 1407, 1492, 1500, 1512, 1534, 1600, 1700, 1777, 1800, 1801, 1809, 1810, 1900, 2000]\n"
     ]
    }
   ],
   "source": [
    "chars = set() \n",
    "for question in data_df[' Question']: \n",
    "    chars.update(question)\n",
    "\n",
    "possible_vals = sorted(set(data_df[ ' Value']))\n",
    "\n",
    "# create mappings\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "print(char_to_idx)\n",
    "# remove newlines!\n",
    "char_to_idx['\\n'] = char_to_idx[' ']\n",
    "\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# printing some stuff\n",
    "print(\"Characters:\", chars)\n",
    "print(\"Number of unique characters:\", vocab_size)\n",
    "print(\"All possible values:\", possible_vals)\n",
    "\n",
    "data_df[' Question'] = data_df[' Question'].apply(lambda x: [char_to_idx[ch] for ch in x]) # converting string to int array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42 \n",
    "train_df = data_df.sample(frac=0.8, random_state=seed)\n",
    "test_df = data_df.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "train_labels = torch.tensor([possible_vals.index(x) for x in train_df[' Value'].tolist()]) # converting values to position in possible values \n",
    "train_questions = [torch.tensor(q) for q in train_df[' Question']]\n",
    "train_questions_padded = pad_sequence(train_questions, batch_first=False, padding_value=44) # char_to_idx[' '] = 44\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.transpose(train_questions_padded, 0, 1), train_labels) \n",
    "\n",
    "\n",
    "test_labels = torch.tensor([possible_vals.index(x) for x in test_df[' Value'].tolist()])\n",
    "test_questions = [torch.tensor(q) for q in test_df[' Question']]\n",
    "test_questions_padded = pad_sequence(test_questions, batch_first=False, padding_value=44)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.transpose(test_questions_padded, 0, 1), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM HW 2\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "ntotal = train_df.shape[0]\n",
    "ntrain = int(0.9*ntotal)\n",
    "nval = ntotal - ntrain\n",
    "\n",
    "val_ix = np.random.choice(range(ntotal), size=nval, replace=False)\n",
    "train_ix = list(set(range(ntotal)) - set(val_ix))\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_ix)\n",
    "val_sampler = SubsetRandomSampler(val_ix)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JeopardyModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(JeopardyModel, self).__init__()\n",
    "        self.embed = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=input_size) \n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.output_layer = torch.nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden_state):\n",
    "        x_vector = self.embed(x)\n",
    "        lstm_outputs, h_n = self.lstm(x_vector, hidden_state)\n",
    "        outputs = self.output_layer(lstm_outputs)         # [:, -1, :]     \n",
    "        return outputs, h_n "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "\n",
    "model = JeopardyModel(input_size=vocab_size, output_size=len(possible_vals), hidden_size=hidden_size)\n",
    "loss_func = torch.nn.CrossEntropyLoss() # torch.nn.MSELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "nepoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 860]) torch.Size([64])\n",
      "torch.Size([64, 46])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.shape, labels.shape)\n",
    "    model = JeopardyModel(input_size=vocab_size, output_size=len(possible_vals), hidden_size=hidden_size)\n",
    "    \n",
    "    batch_size = images.size(1)\n",
    "    h = torch.zeros((2, batch_size, hidden_size)) \n",
    "    c = torch.zeros((2, batch_size, hidden_size))\n",
    "    \n",
    "    outputs, _ = model(images, (h,c))\n",
    "    print(outputs.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM HW 2\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_network(model, train_loader, val_loader, criterion, optimizer, nepoch):\n",
    "    try:\n",
    "        for epoch in tqdm(range(nepoch)):\n",
    "            print('EPOCH %d'%epoch)\n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "\n",
    "            h = torch.zeros((2, batch_size,hidden_size))    \n",
    "            c = torch.zeros((2, batch_size,hidden_size))\n",
    "            for inputs, labels in train_loader:\n",
    "                if inputs.shape[0] < 64: continue # dropping last batch\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs, (h,c) = model(inputs , (h.detach(), c.detach()))\n",
    "                # print(\"o\", outputs)\n",
    "                # print(\"l\", labels)\n",
    "                loss = criterion(outputs, labels.long())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # print(loss.item())\n",
    "                total_loss += loss.item()\n",
    "                count += 1\n",
    "            print('{:>12s} {:>7.5f}'.format('Train loss:', total_loss/count))\n",
    "            with torch.no_grad():\n",
    "                total_loss = 0\n",
    "                count = 0\n",
    "\n",
    "                h = torch.zeros((2, batch_size,hidden_size))  \n",
    "                c = torch.zeros((2, batch_size,hidden_size))\n",
    "                for inputs, labels in val_loader:\n",
    "                    if inputs.shape[0] < 64: continue # dropping last batch\n",
    "                    outputs, (h,c) = model(inputs , (h.detach(), c.detach()))\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    total_loss += loss.item()\n",
    "                    count += 1\n",
    "                print('{:>12s} {:>7.5f}'.format('Val loss:', total_loss/count))\n",
    "            print()\n",
    "    except KeyboardInterrupt:\n",
    "        print('Exiting from training early')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ddcfc3681a49968c931ce0b4f965f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n"
     ]
    }
   ],
   "source": [
    "train_network(model, train_loader, val_loader, loss_func, optimizer, nepoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network(model, test_loader, mode):\n",
    "    true, pred = [], []\n",
    "    total = correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels  in test_loader:\n",
    "            batch = inputs.shape[0]\n",
    "            h = torch.zeros((2, batch,hidden_size))    \n",
    "            c = torch.zeros((2, batch,hidden_size))\n",
    "\n",
    "            outputs, _  = model(inputs, (h,c))\n",
    "            print(outputs)\n",
    "            predicted = np.argmax(outputs, axis=1) # get predicted class label for each test example.\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            true.append(labels)\n",
    "            pred.append(predicted)\n",
    "    acc = (100 * correct / total)\n",
    "    print('%s accuracy: %0.3f' % (mode, acc))\n",
    "    true = np.concatenate(true)\n",
    "    pred = np.concatenate(pred)\n",
    "    return acc, true, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0408, -0.0106,  0.0221, -0.0656,  0.0046, -0.0661,  0.0051, -0.0721,\n",
      "          0.0510, -0.0113, -0.0256,  0.0492,  0.0548, -0.0315]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "tensor([[-0.0232,  0.0044,  0.0162, -0.0416,  0.0149, -0.0763, -0.0241,  0.0055,\n",
      "         -0.0347,  0.0360, -0.0194,  0.0161,  0.0424,  0.0331]])\n",
      "Model accuracy: 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " array([ 1,  1,  5,  5,  6,  6,  7,  3,  3,  6,  9, 11, 13,  1,  3,  3,  5,\n",
       "         7,  3,  3,  6,  6,  9,  9, 11, 11, 13,  1,  2,  3,  3,  4,  5,  6,\n",
       "         7,  6,  7,  0,  3,  3,  3,  1,  1,  3,  3,  5,  5,  6,  1,  3,  5,\n",
       "         5,  5,  6,  6,  6,  7,  9,  9, 13,  3,  6,  7,  7,  6, 11, 11, 13,\n",
       "        13,  3,  4,  1,  1,  3,  3,  3,  5,  5,  8,  6,  7,  1,  3,  6,  3,\n",
       "        13, 13, 13, 13,  1,  1,  3,  5,  5,  5,  5,  7,  3,  3,  3,  6,  9,\n",
       "         9, 11, 11,  0,  1,  3,  4,  1,  3,  3,  5,  5,  5, 13,  5,  5,  6,\n",
       "         7,  7,  1,  5,  7,  6,  6,  1,  1,  2, 10,  4,  4,  3,  5,  6,  6,\n",
       "         7,  1,  1,  1,  3,  3,  7,  7,  3,  3,  3,  3,  6,  9,  9, 11, 13,\n",
       "         3,  3,  5,  5,  5,  5,  6,  6,  7,  7,  6,  6, 11, 11,  1,  1,  3,\n",
       "         3,  7,  5,  7,  7,  3,  6, 11, 11, 13, 13,  0,  0,  1,  2,  1,  3,\n",
       "        13,  7,  4,  1,  1,  3,  6,  4]),\n",
       " array([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 12, 12, 12, 12, 12, 12, 12]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, true, pred = test_network(model, test_loader, \"Model\")\n",
    "acc, true, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
    "        self.decoder = nn.Linear(d_model, len(possible_vals))\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = tgt.unsqueeze(1).unsqueeze(-1).expand(-1, src.size(1), src.size(2))\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, dataloader, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs, labels.float())\n",
    "        loss = criterion(output[:,-1,:], labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, criterion, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            output = model(inputs, labels.float())\n",
    "            loss = criterion(output[:,-1,:], labels)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chars' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mchars\u001b[49m)  \u001b[38;5;66;03m# Example vocab size, replace with actual vocab size\u001b[39;00m\n\u001b[1;32m      2\u001b[0m d_model \u001b[38;5;241m=\u001b[39m vocab_size \u001b[38;5;66;03m# 32  # Dimension of the model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m nhead \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m  \u001b[38;5;66;03m# Number of attention heads\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chars' is not defined"
     ]
    }
   ],
   "source": [
    "vocab_size = len(chars)  # Example vocab size, replace with actual vocab size\n",
    "d_model = vocab_size # 32  # Dimension of the model\n",
    "nhead = 4  # Number of attention heads\n",
    "num_encoder_layers = 2  # Number of encoder layers\n",
    "num_decoder_layers = 2  # Number of decoder layers\n",
    "dim_feedforward = 128  # Dimension of the feedforward network\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "model = TransformerModel(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Step 3: Train and test the model\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    train_loss = train(model, criterion, optimizer, train_loader, 'cpu')\n",
    "    print(\"train loss = \", train_loss)\n",
    "    test_loss = evaluate(model, criterion, test_loader, 'cpu')\n",
    "    print(\"test loss = \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network(model, test_loader, mode):\n",
    "    true, pred = [], []\n",
    "    total = correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels  in test_loader:\n",
    "\n",
    "            outputs = model(inputs, labels.float())\n",
    "            predicted = np.argmax(outputs, axis=1) # get predicted class label for each test example.\n",
    "            # predicted = outputs[:, -1, :]\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            true.append(labels)\n",
    "            pred.append(predicted)\n",
    "    acc = (100 * correct / total)\n",
    "    print('%s accuracy: %0.3f' % (mode, acc))\n",
    "    true = np.concatenate(true)\n",
    "    pred = np.concatenate(pred)\n",
    "    return acc, true, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, true, pred = test_network(model, test_loader, \"Model\")\n",
    "acc, true, pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
